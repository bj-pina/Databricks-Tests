{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0535a448-21ae-472d-af82-42d9086eaf86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Libraries and variables setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43b5c43-0686-4ff9-be66-665b015bb545",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Variables\n",
    "\n",
    "socrata_token = dbutils.secrets.get(scope = \"bpina_secrets\", key = \"socrata_api_token\")\n",
    "api_url = 'https://evergreen.data.socrata.com/resource/c53k-p9dd.json'\n",
    "chunk_size = 50000\n",
    "max_dataset_size = 130000\n",
    "num_tasks =( max_dataset_size + chunk_size - 1) // chunk_size \n",
    "output_path = \"dbfs:/external_data/seattle_911_raw_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d59d12d-2528-4b13-989a-3c30aac71c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def parallel_api_ingestion(api_url, base_params_list, output_dbfs_dir):\n",
    "\n",
    "    hadoop_config = spark.sparkContext._jsc.hadoopConfiguration()\n",
    "    fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(hadoop_config)\n",
    "\n",
    "    def fetch_and_write_socrata_data(task):\n",
    "        \"\"\"Fetches data from the Socrata API for a given set of parameters.\"\"\"\n",
    "        import requests\n",
    "        params = task  # Task is a dictionary of parameters\n",
    "        try:\n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "            filename = f\"api_data_{timestamp}_{hash(tuple(sorted(params.items())))}.json\"\n",
    "            file_path_uri = f\"{output_dbfs_path}{filename}\"\n",
    "            path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(file_path_uri)\n",
    "\n",
    "            output_stream = fs.create(path, True) # True for overwrite\n",
    "            output_stream.write(bytearray(json.dumps(data), 'utf-8'))\n",
    "            output_stream.close()\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching data for {params}: {e}\")\n",
    "            return False\n",
    "        except ValueError:\n",
    "            print(f\"Error decoding JSON for {params}\")\n",
    "            return False\n",
    "    \n",
    "    try:\n",
    "        dbutils.fs.ls(\"dbfs:/external_data/seattle_911_raw_data/\")\n",
    "    except Exception as e:\n",
    "        dbutils.fs.mkdirs(output_dbfs_dir)\n",
    "        print(f\"Created directory: {output_dbfs_dir}\")\n",
    "\n",
    "    params_rdd = spark.sparkContext.parallelize(base_params_list)\n",
    "    results_rdd = params_rdd.map(fetch_and_write_socrata_data)\n",
    "    successful_writes = results_rdd.filter(lambda x: x).count()\n",
    "    total_calls = params_rdd.count()\n",
    "\n",
    "    print(f\"Attempted {total_calls} API calls. {successful_writes} were successful.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8e4843f-73c2-464e-8af8-6b2afe8166a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of dictionaries with the corresponding parameters\n",
    "list_of_params = []\n",
    "\n",
    "for i in range(num_tasks):\n",
    "    list_of_params.append({\n",
    "        \"$limit\": chunk_size,\n",
    "        \"$$app_token\": socrata_token,\n",
    "        \"$offset\": i * chunk_size\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5833a10-ca08-4b9e-b23b-9fbd4c1e94be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parallel_api_ingestion(api_url,list_of_params,output_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1 - Raw Ingestion Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
